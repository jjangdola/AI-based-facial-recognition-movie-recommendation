{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICG7hVLfkjFv",
        "outputId": "491fcd69-6636-4510-a5a0-209023a5c03b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "fonts-nanum is already the newest version (20170925-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 20 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "# 한글 폰트 설치하기 (꼭! 설치가 완료되면 [런타임 다시 시작]을 누르고 다시 실행하기)\n",
        "!apt install fonts-nanum -y\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "# 한글 폰트 설정하기\n",
        "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
        "font = fm.FontProperties(fname=fontpath, size=10)\n",
        "plt.rc('font', family='NanumBarunGothic')\n",
        "matplotlib.font_manager._rebuild()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 라이브러리 만들기\n",
        "!git clone https://github.com/ndb796/bing_image_downloader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPuJC6p5k4PV",
        "outputId": "988f5dfc-e425-492f-da74-72427b302978"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'bing_image_downloader'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from bing_image_downloader.bing_image_downloader import downloader\n",
        "\n",
        "\n",
        "directory_list = [\n",
        "    './custom_dataset/train/',\n",
        "    './custom_dataset/test/',\n",
        "]\n",
        "\n",
        "# 초기 디렉토리 만들기\n",
        "for directory in directory_list:\n",
        "    if not os.path.isdir(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "# 수집한 이미지를 학습 데이터와 평가 데이터로 구분하는 함수\n",
        "def dataset_split(query, train_cnt):\n",
        "    # 학습 및 평가 데이터셋 디렉토리 만들기\n",
        "    for directory in directory_list:\n",
        "        if not os.path.isdir(directory + '/' + query):\n",
        "            os.makedirs(directory + '/' + query)\n",
        "    # 학습 및 평가 데이터셋 준비하기\n",
        "    cnt = 0\n",
        "    for file_name in os.listdir(query):\n",
        "        if cnt < train_cnt:\n",
        "            print(f'[Train Dataset] {file_name}')\n",
        "            shutil.move(query + '/' + file_name, './custom_dataset/train/' + query + '/' + file_name)\n",
        "        else:\n",
        "            print(f'[Test Dataset] {file_name}')\n",
        "            shutil.move(query + '/' + file_name, './custom_dataset/test/' + query + '/' + file_name)\n",
        "        cnt += 1\n",
        "    shutil.rmtree(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "qmvjE424k8sm",
        "outputId": "380d135c-3aa9-4b59-fbf5-38174158f0b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-70866487bab7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbing_image_downloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbing_image_downloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bing_image_downloader'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = '윤아'\n",
        "downloader.download(query, limit=60,  output_dir='./', adult_filter_off=True, force_replace=False, timeout=60)\n",
        "dataset_split(query, 40)"
      ],
      "metadata": {
        "id": "zclA_fiPlFJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = '한지민'\n",
        "downloader.download(query, limit=60,  output_dir='./', adult_filter_off=True, force_replace=False, timeout=60)\n",
        "dataset_split(query, 40)"
      ],
      "metadata": {
        "id": "c6HCELkelO_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = '이동욱'\n",
        "downloader.download(query, limit=60,  output_dir='./', adult_filter_off=True, force_replace=False, timeout=60)\n",
        "dataset_split(query, 40)"
      ],
      "metadata": {
        "id": "tJ8XYolUlQju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = '강하늘'\n",
        "downloader.download(query, limit=60,  output_dir='./', adult_filter_off=True, force_replace=False, timeout=60)\n",
        "dataset_split(query, 40)"
      ],
      "metadata": {
        "id": "wza2SPrYlT9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = '이광수'\n",
        "downloader.download(query, limit=60,  output_dir='./', adult_filter_off=True, force_replace=False, timeout=60)\n",
        "dataset_split(query, 40)"
      ],
      "metadata": {
        "id": "0vuKRBBNlVkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = '서강준'\n",
        "downloader.download(query, limit=60,  output_dir='./', adult_filter_off=True, force_replace=False, timeout=60)\n",
        "dataset_split(query, 40)"
      ],
      "metadata": {
        "id": "m7aGjnWplWx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 라이브러리 불러오기\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # device 객체"
      ],
      "metadata": {
        "id": "TzFA-L33lcBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋을 불러올 때 사용할 변형(transformation) 객체 정의\n",
        "transforms_train = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(), # 데이터 증진(augmentation)\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # 정규화(normalization)\n",
        "])\n",
        "\n",
        "transforms_test = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "data_dir = './custom_dataset'\n",
        "train_datasets = datasets.ImageFolder(os.path.join(data_dir, 'train'), transforms_train)\n",
        "test_datasets = datasets.ImageFolder(os.path.join(data_dir, 'test'), transforms_test)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(train_datasets, batch_size=4, shuffle=True, num_workers=4)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_datasets, batch_size=4, shuffle=True, num_workers=4)\n",
        "\n",
        "print('Train_Datasets_Size :', len(train_datasets))\n",
        "print('Test_Datasets_Size :', len(test_datasets))\n",
        "\n",
        "class_names = train_datasets.classes\n",
        "print('Class:', class_names)"
      ],
      "metadata": {
        "id": "d5U7wDCZl9yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지 시각화\n",
        "def imshow(input, title):\n",
        "    # torch.Tensor를 numpy 객체로 변환\n",
        "    input = input.numpy().transpose((1, 2, 0))\n",
        "    # 이미지 정규화 해제하기\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    input = std * input + mean\n",
        "    input = np.clip(input, 0, 1)\n",
        "    # 이미지 출력\n",
        "    plt.imshow(input)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# 학습 데이터를 배치 단위로 불러오기\n",
        "iterator = iter(train_dataloader)\n",
        "\n",
        "# 현재 배치를 이용해 격자 형태의 이미지를 만들어 시각화\n",
        "inputs, classes = next(iterator)\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "imshow(out, title=[class_names[x] for x in classes])"
      ],
      "metadata": {
        "id": "0lHfqw2_mDWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CNN 딥러닝 모델 객체를 초기화\n",
        "model = models.resnet34(pretrained=True)\n",
        "num_features = model.fc.in_features\n",
        "# 전이 학습(transfer learning): 모델의 출력 뉴런 수를 6개로 교체하여 마지막 레이어 다시 학습\n",
        "model.fc = nn.Linear(num_features, 6)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "rHXrFeU8mm2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습을 진행\n",
        "num_epochs = 50\n",
        "model.train()\n",
        "start_time = time.time()\n",
        "\n",
        "# 전체 반복(epoch) 수 만큼 반복하며\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.\n",
        "    running_corrects = 0\n",
        "\n",
        "    # 배치 단위로 학습 데이터 불러오기\n",
        "    for inputs, labels in train_dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # 모델에 입력(forward)하고 결과 계산\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # 역전파를 통해 기울기(gradient) 계산 및 학습 진행\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_datasets)\n",
        "    epoch_acc = running_corrects / len(train_datasets) * 100.\n",
        "\n",
        "    # 학습 과정 중에 결과 출력\n",
        "    print('#{} Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() - start_time))"
      ],
      "metadata": {
        "id": "wCWh6zEFmvgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 평가\n",
        "model.eval()\n",
        "start_time = time.time()\n",
        "\n",
        "with torch.no_grad():\n",
        "    running_loss = 0.\n",
        "    running_corrects = 0\n",
        "\n",
        "    for inputs, labels in test_dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        # 한 배치의 첫 번째 이미지에 대하여 결과 시각화\n",
        "        print(f'[예측 분석: {class_names[preds[0]]}] (정답: {class_names[labels.data[0]]})')\n",
        "        imshow(inputs.cpu().data[0], title='예측 분석: ' + class_names[preds[0]])\n",
        "\n",
        "    epoch_loss = running_loss / len(test_datasets)\n",
        "    epoch_acc = running_corrects / len(test_datasets) * 100.\n",
        "    print('[Test Phase] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch_loss, epoch_acc, time.time() - start_time))"
      ],
      "metadata": {
        "id": "LTlq96GSmysv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트용 이미지 다운로드하기\n",
        "!wget https://upload.wikimedia.org/wikipedia/commons/f/f1/Don_Lee_by_Gage_Skidmore.jpg -O test_image.jpg"
      ],
      "metadata": {
        "id": "S8VOmRjcm3mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "\n",
        "image = Image.open('test_image.jpg')\n",
        "image = transforms_test(image).unsqueeze(0).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(image)\n",
        "    _, preds = torch.max(outputs, 1)\n",
        "    imshow(image.cpu().data[0], title='예측 분석: ' + class_names[preds[0]])"
      ],
      "metadata": {
        "id": "mOVLv8Vgm77M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 라이브러리 설치하기\n",
        "!pip install flask-ngrok"
      ],
      "metadata": {
        "id": "nOye7_YRnAx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask, jsonify, request\n",
        "\n",
        "\n",
        "# 이미지를 읽어 결과를 반환하는 함수\n",
        "def get_prediction(image_bytes):\n",
        "    image = Image.open(io.BytesIO(image_bytes))\n",
        "    image = transforms_test(image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        imshow(image.cpu().data[0], title='예측 분석: ' + class_names[preds[0]])\n",
        "\n",
        "    return class_names[preds[0]]\n",
        "\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "\n",
        "@app.route('/', methods=['POST'])\n",
        "def predict():\n",
        "    if request.method == 'POST':\n",
        "        # 이미지 바이트 데이터 받아오기\n",
        "        file = request.files['file']\n",
        "        image_bytes = file.read()\n",
        "\n",
        "        # 분류 결과 확인 및 클라이언트에게 결과 반환\n",
        "        class_name = get_prediction(image_bytes=image_bytes)\n",
        "        print(\"결과:\", {'class_name': class_name})\n",
        "        return jsonify({'class_name': class_name})"
      ],
      "metadata": {
        "id": "fS_US_PFnFOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_with_ngrok(app)\n",
        "app.run()"
      ],
      "metadata": {
        "id": "lhlp69hSnHOB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}